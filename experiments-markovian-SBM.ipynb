{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference in the Stochastic Block Model with a Markovian assignment of the communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import markovianSBM\n",
    "from markovianSBM.SBM import SBM\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "\n",
    "The Stochastic Block Model (SBM) is a famous random graph model where a community in $[K]=\\{1, \\dots , K\\}$ (where $K\\geq 2$ is an integer) is assigned to each node $i \\in [n]$ of a graph. Once the communities have been assigned, an edge between a given pair of vertices will be present in the graph with a probabililty that only depends of the communities of the two nodes considered. \\\\ We propose a new extension of the Stochastic Block Model where the communities of the nodes in the graph are assigned through a Markovian dynamic. In our paper, we provide a partial recovery bound : We prove that the algorithm leads to a classification error that decays exponentially fast with respect to an appropriate signal to noise ratio (SNR). We provide consistent estimation of the parameters of our model when the average degree of the nodes is of order $\\log(n)$ or higher.\n",
    "\n",
    "\n",
    "In the following, we give the reader the opportunity to understand how to use our algorithm that we have implemented\n",
    "and to reproduce results that have been presented in our paper.\n",
    "\n",
    "\n",
    "# 1) Model\n",
    "\n",
    "\n",
    "We start by ordering the $n$ nodes in $V$ and without loss of generality, we consider the increasing order of the integers $1,2, \\dots,n$. For all $i \\in [n]$, we denote $C_i \\in [K]$ the random variable representing the community of the node $i$ and we consider that they satistify the following assumption. $(C_i)_{i  \\in [n]}$ is a positive recurrent Markov chain on the finite space $[K]$ with invariant probability $\\pi$, with transition matrix $P \\in \\mathbb{R}^{K \\times K}$ and initial distribution $\\pi$. \n",
    "\n",
    "\\medskip\n",
    "\n",
    "We assign communities as follows:\n",
    "\\begin{align*}\n",
    "&C_1 \\sim \\pi \\\\\n",
    "&\\text{For }\\;i=1\\; \\dots\\; (n-1) \\; \\text{ Do} \\\\\n",
    "&\\qquad C_{i+1} \\sim P_{C_{i},:}.\\\\\n",
    "& \\text{EndFor.}\n",
    "\\end{align*}\n",
    "\n",
    "Once the community of each node is assigned, we draw an edge between the nodes $i$ and $j$ with probability $Q_{C_i,C_j}$\n",
    "$$X_{i,j} \\sim \\text{Ber}(Q_{C_i,C_j}) \\quad \\text{ with }\\quad Q:=\\alpha_n Q_0.$$\n",
    "\n",
    "Here, $Q_0 \\in [0,1]^{K \\times K}$ and $\\alpha_n \\in (0,1)$  is varying with $n$. \n",
    "\n",
    "---\n",
    "\n",
    "Given two partitions $\\hat{G}= (\\hat{G}_1, \\dots , \\hat{G}_K)$ and $G = (G_1, \\dots ,G_K)$ of $[n]$ into $K$ non-void groups, we define the proportion of non-matching points $$\\text{err}(\\hat{G},G) = \\min_{\\sigma \\in \\mathcal{S}_K} \\frac{1}{2n} \\sum_{k=1}^K \\left|\n",
    "\\hat{G}_k \\; \\Delta \\; G_{\\sigma(k)} \\right| ,$$\n",
    "\n",
    "where $A\\;\\Delta \\; B$ represents the symmetric difference between the two sets $A$ and $B$ and $\\mathcal{S}_K$\n",
    "represents the set of permutations on $\\{1, \\dots ,K\\}$. When $\\hat{G}$ is a partition estimating $G$, we\n",
    "refer to $\\text{err}(\\hat{G},G)$ as the misclassification proportion (or error) of the clustering.\n",
    "\n",
    "\n",
    "# 2) Algorithm\n",
    "\n",
    "\n",
    "Peng and Wei in [PW07](http://www.optimization-online.org/DB_FILE/2005/04/1114.pdf) showed that any partition $G$ of $[n]$ can be uniquely represented by a $n \\times n$ matrix $B^* \\in \\mathbb{R}^{n\\times n}$ defined by \n",
    "\n",
    "\\begin{equation}(*_1)\\qquad \\forall i,j \\in [n], \\quad B_{i,j}^*=\\left\\{\n",
    "    \\begin{array}{ll} \\displaystyle\n",
    "        \\frac{1}{m_k}& \\mbox{if } i \\text{ and } j \\text{ belong to the same community } k \\in [K] \\\\\n",
    "        0 & \\text{ otherwise.}\n",
    "    \\end{array}\n",
    "\\right. \\end{equation}\n",
    "\n",
    "The set of such matrices $B^*$ that can be built from a particular partition of $[n]$ in $K$ groups is defined by \n",
    "\n",
    "$$\\mathcal{S}=\\{B\\in \\mathbb{R}^{n \\times n}:  \\text{ symmetric}, \\; B^2=B, \\; \\text{Tr}(B) =K, \\; B\\textbf{1} =  \\textbf{1}, \\;  B\\geq0\\},$$\n",
    "\n",
    "where $\\textbf{1} \\in \\mathbb{R}^n$ is the $n$-dimensional vector with all entries equal to one and where $B \\geq 0$ means that all entries of $B$ are nonnegative. Peng and Wei [PW07](http://www.optimization-online.org/DB_FILE/2005/04/1114.pdf) proved that solving the $K$-means problem\n",
    "\n",
    "$$(*_2) \\qquad \\text{Crit}(G) = \\sum_{k=1}^K \\sum_{i \\in G_k} \\Bigg\\| X_{:,i} - \\frac{1}{|G_k|}\\sum_{j \\in G_k} X_{:,j}  \\Bigg\\|^2  ,$$\n",
    "\n",
    "\n",
    "is equivalent to \\begin{equation} \\underset{B \\in \\mathcal{S}}{\\max} \\; \\langle XX^{\\top}, B \\rangle. \\label{Kmeans}\\end{equation} \n",
    "\n",
    "Writing $B^*$ an optimal solution of $(*_2)$, an optimal solution for the $K$-means problem is obtained by gathering indices $i,j \\in [n]$ such that $B^*_{i,j}\\neq 0$. The set $\\mathcal{S}$ is not convex and the authors of [GV18](https://arxiv.org/abs/1807.07547) propose the following relaxation of problem $(*_2)$\n",
    "\n",
    "\\begin{equation} \\hat{B} \\in \\underset{B \\in \\mathcal{C}_{\\beta}}{\\arg \\max} \\; \\langle XX^{\\top},B \\rangle, \\label{relaxed-SDP}\\end{equation}\n",
    "\n",
    "\n",
    "where\n",
    "$$(*_3) \\qquad \\mathcal{C}_{\\beta}:=\\{B\\in \\mathbb{R}^{n \\times n}:  \\text{ symmetric}, \\; \\text{Tr}(B) =K, \\; B\\textbf{1} =  \\textbf{1}, 0 \\leq B \\leq \\beta\\} \\quad \\text{ with } \\quad  K/n \\leq \\beta \\leq 1. $$\n",
    "\n",
    "\n",
    "At this step, we cannot ensure that $\\hat{B}$ belongs to $\\mathcal{S}$ and a final refinement is necessary to end up with a clustering of the nodes of the graph. This final rounding step is achieved by running a $K$-medoid algorithm on the rows of $\\hat{B}$. Given a partition $\\{G_1, \\dots , G_k\\}$ of the $n$ nodes of the graph into $K$ communities, we define the related membership matrix $A \\in \\mathbb{R}^{n \\times K}$ where $A_{i,k} = \\mathbb{1}_{i \\in G_k}$.\n",
    "\n",
    "Working on the rows of $\\hat{B}$, a $K$-medoid algorithm tries to find efficiently a pair $(\\hat{A},\\hat{M})$ with $\\hat{A} \\in \\mathcal{A}_K$, $\\hat{M} \\in \\mathbb{R}^{K \\times n}$, $\\text{Rows}(\\hat{M}) \\subset \\text{Rows}(\\hat{B})$ satisfying for some $\\rho >0$\n",
    "\\begin{equation}(*_4) \\qquad |\\hat{A}\\hat{M}-\\hat{B}|_1 \\leq \\rho \\underset{A \\in \\mathcal{A}_K, \\text{Rows}(M) \\subset \\text{Rows}(\\hat{B})}{\\min}\\;|AM-\\hat{B}|_1,\\label{medoid}\\end{equation}\n",
    "where ${A}_K$ is the set of all possible membership matrices and  $\\text{Rows}(\\hat{B})$ the set of all rows of $\\hat{B}$.\n",
    "\n",
    "We end up with the following algorithm.\n",
    "\n",
    "\n",
    "**Algorithm Description**\n",
    "\n",
    "1. Compute the density of the graph $d_{X}=\\frac{2|E|}{n(n-1)}$ and set $\\hat{\\beta} = \\frac{K^3}{n}e^{2nd_X}\\wedge 1$.\n",
    "2. Find $\\hat{B} \\in  \\underset{B \\in \\mathcal{C}_{\\hat{\\beta}}}{\\arg \\max} \\; \\langle XX^{\\top},B \\rangle$ (using for example the interior-point method).\n",
    "3. Run the $K$-medoids algorithm from [CGTS02](https://www.sciencedirect.com/science/article/pii/S0022000002918829)  on the rows of $\\hat{B}$. Note $\\hat{A} \\in \\{0,1\\}^{n \\times K}$ the membership matrix obtained.\n",
    "4. Define $\\forall k \\in [K], \\quad \\hat{G}_k = \\{ i\\in [n] \\; : \\; \\hat{A}_{i,k}=1 \\}$ and $\\forall i \\in [n], \\quad \\hat{C}_i=k$ where $k \\in [K]$ is such that $\\hat{A}_{i,k}=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Reproducing results of our paper\n",
    "\n",
    "### 4.1) $2$ communities : Estimation of the transition matrix of the Markov chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nb_experiments = 1\n",
    "\n",
    "P = np.array([[0.2, 0.8],[0.6, 0.4]])\n",
    "Q = np.array([[0.8, 0.2],[0.1, 0.3]])\n",
    "\n",
    "norminfP = np.zeros(n)\n",
    "scores = np.zeros(n)\n",
    "sizes = np.zeros(n)\n",
    "nodes = [i*10 for i in range(1,n+1)]\n",
    "results = {'nodes':[], 'norm':[], 'score':[]}\n",
    "for i in range(1,n+1):\n",
    "    G = SBM(n,K, framework='markov', P=P, Q=(40/n)*Q)\n",
    "    G.estimate_partition()\n",
    "    G.estimate_parameters()\n",
    "    score = 100*G.proportion_error()\n",
    "    count = 0\n",
    "    for j in range(nb_experiments):\n",
    "        norminfP[i-1] += np.max(np.abs(G.approx_P-P))\n",
    "        results['norm'].append(np.max(np.abs(G.approx_P-P)))\n",
    "        results['nodes'].append(i)\n",
    "        results['score'].append(score[j])\n",
    "        count += 1\n",
    "        sizes[i-1]+=1\n",
    "    norminfP[i-1] /= count\n",
    "    scores[i-1] /= count\n",
    "\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "dflog = df.apply(np.log10)\n",
    "ax = sns.lineplot(x=\"nodes\", y=\"norm\", err_style=\"bars\", data=dflog)\n",
    "ax.set_ylabel('$\\log(\\|\\|\\hat{P}-P\\|\\|_{\\infty})$',fontsize=12)\n",
    "ax.set_xlabel('$\\log(n)$',fontsize=12)\n",
    "ax.set_title('Consistency of the estimation of the transition matrix')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2) $5$ communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "P = np.array([[0.1, 0.3, 0.5,0.01, 0.09],[0.55, 0.15, 0.1, 0.05,0.15],[0.15, 0.3, 0.1, 0.2,0.25 ],[0.15, 0.05, 0.1, 0.5,0.2 ],[0.2,0.3,0.1,0.05,0.35]])\n",
    "Q = np.array([[0.6, 0.1, 0.15, 0.1, 0.2 ],[0.2, 0.5, 0.35, 0.1, 0.4 ], [0.4, 0.15, 0.6, 0.25, 0.05], [0.4,0.1,0.1,0.2,0.55],[0.3,0.35,0.2,0.1,0.7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1) Visualization of the matrices $\\hat{B}$ and $B^*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$n=40$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 40\n",
    "G = SBM(n, K, framework='markov', P=P, Q=Q)\n",
    "G.estimate_partition()\n",
    "G.visualize_B_matrices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$n=160$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 160\n",
    "G = SBM(n, K, framework='markov', P=P, Q=Q)\n",
    "G.estimate_partition()\n",
    "G.visualize_B_matrices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2) Recovery of hidden variables and parameter estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsP = []\n",
    "lsQ = []\n",
    "lspi = []\n",
    "lsscore = []\n",
    "sizes = [40,60,80,100,120,140,160,180]\n",
    "for n in sizes:\n",
    "    G = SBM(n,K, framework='markov', P=P, Q=Q)\n",
    "    G.estimate_partition()\n",
    "    G.estimate_parameters()\n",
    "    score= 100*G.proportion_error()\n",
    "    lsP.append(G.approx_P)\n",
    "    lsQ.append(G.approx_Q)\n",
    "    lspi.append(G.approx_pi)\n",
    "    lsscore.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Missclassification error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lsn,np.log10(np.array(lsscore)/np.array(sizes)))\n",
    "plt.ylabel('$ \\log_{10}(\\mathrm{err}(\\hat{G},G))$',fontsize=12)\n",
    "plt.xlabel('Number of nodes: $n$',fontsize=12)\n",
    "plt.title('Convergence rate of the misclassification error for $K=5$.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Concistency of the estimation of the transition and connectivity matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infP = []\n",
    "infQ = []\n",
    "lsP = np.array(lsP)\n",
    "for i in range(lsP.shape[0]):\n",
    "    infP.append(np.max(np.abs(P-lsP[i])))\n",
    "    infQ.append(np.max(np.abs(Q-lsQ[i])))\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "ax.plot(np.log10(sizes),np.log10(infP))\n",
    "ax.set_ylabel('$ \\log_{10}(\\mathrm{err}(\\hat{G},G))$',fontsize=12)\n",
    "ax.set_xlabel('$\\log(n)$',fontsize=12)\n",
    "ax.set_title('Consistency of the estimation of the transition matrix for $K=5$.')\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "ax.plot(np.log10(sizes),np.log10(infQ))\n",
    "ax.set_ylabel('$ \\log_{10}(\\mathrm{err}(\\hat{G},G))$',fontsize=12)\n",
    "ax.set_xlabel('$\\log(n)$',fontsize=12)\n",
    "ax.set_title('Consistency of the estimation of the connectivity matrix for $K=5$.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
